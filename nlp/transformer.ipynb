{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transformers from Scratch in PyTorch\n",
    "# https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "\n",
    "# Transformers in Pytorch from scratch for NLP Beginners\n",
    "# https://hyugen-ai.medium.com/transformers-in-pytorch-from-scratch-for-nlp-beginners-ff3b3d922ef7\n",
    "\n",
    "# The Illustrated Transformer\n",
    "# https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "# Language Modeling with nn.Transformer and TorchText\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    scores = query.bmm(key.transpose(1, 2))\n",
    "    scores /= key.size(-1) ** 0.5\n",
    "\n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "\n",
    "    softmax = F.softmax(scores, dim=-1)\n",
    "    return softmax.bmm(value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, dim_k, dim_v):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "        return scaled_dot_product_attention(q, k, v, mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_in, dim_k, dim_v):\n",
    "        super().__init__()\n",
    "        heads = [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        self.liner = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        x = [head(query, key, value, mask) for head in self.heads]\n",
    "        x = torch.cat(x, dim=-1)\n",
    "        x = self.liner(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def positional_encoding(seq_len, dim_model, device):\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / 1e4 ** (dim / dim_model)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def feed_forward(dim_input, dim_feedforward):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer, dim, dropout):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors, **kwargs):\n",
    "        x = self.sublayer(*tensors, **kwargs)\n",
    "        x = self.dropout(x)\n",
    "        x += tensors[-1]\n",
    "        x = self.norm(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dim_model, dropout\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dim_model, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(x, x, x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, dim_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(num_heads, dim_model, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, dimension = x.size(1), x.size(2)\n",
    "        x += positional_encoding(seq_len, dimension, device=x.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dim_model, dropout\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
    "            dim_model, dropout\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dim_model, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, y, enc, mask):\n",
    "        y = self.attention_1(y, y, y, mask=mask)\n",
    "        y = self.attention_2(y, enc, enc)\n",
    "        y = self.feed_forward(y)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, dim_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(num_heads, dim_model, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, y, enc):\n",
    "        seq_len, dimension = y.size(1), y.size(2)\n",
    "        y += positional_encoding(seq_len, dimension, device=y.device)\n",
    "\n",
    "        mask = torch.full((seq_len, seq_len), float('-inf'), device=y.device).triu(1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            y = layer(y, enc, mask)\n",
    "\n",
    "        y = self.linear(y)\n",
    "        y = torch.softmax(y, dim=-1)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size = 40000, num_encoder_layers = 6, num_decoder_layers = 6,\n",
    "                dim_model = 512, num_heads = 8, dim_feedforward = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.encoder = Encoder(num_encoder_layers, num_heads, dim_model, dim_feedforward, dropout)\n",
    "        self.decoder = Decoder(num_decoder_layers, num_heads, vocab_size, dim_model, dim_feedforward, dropout)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.embedding(x)\n",
    "        y = self.embedding(y)\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "        return self.decoder(y, enc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    import torchtext\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    specials = ['<unk>', '<bos>', '<eos>']\n",
    "\n",
    "    # Load the dataset and create the tokenizer\n",
    "    train = torchtext.datasets.WikiText2(split='train')\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train), specials=specials)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "    unk, bos, eos = vocab(specials)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Data pipeline\n",
    "    def generate_data(seq):\n",
    "        x = torch.tensor(seq + [eos], device=device)\n",
    "        y = torch.tensor([bos] + seq, device=device)\n",
    "        return x, y\n",
    "\n",
    "    def preprocess(dataset):\n",
    "        dataset = map(lambda seq: vocab(tokenizer(seq)), dataset)\n",
    "        dataset = filter(lambda seq: len(seq) > 0, dataset)\n",
    "        dataset = list(map(generate_data, dataset))\n",
    "        return DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    train = torchtext.datasets.WikiText2(split='train')\n",
    "    train = preprocess(train)\n",
    "\n",
    "    # Create a model\n",
    "    model = Transformer(vocab_size).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "    # Train it!\n",
    "    for epoch in range(5):\n",
    "        _loss = 0.0\n",
    "        for x, y in train:\n",
    "            pred = model(x, y)\n",
    "            loss = loss_fn(pred.squeeze(), x.squeeze())\n",
    "            _loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch {}, loss {} '.format(epoch, _loss))\n",
    "\n",
    "    # Save the vocab and model\n",
    "    torch.save(vocab, 'vocab.pt')\n",
    "    torch.save(model, 'transformer.pt')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}